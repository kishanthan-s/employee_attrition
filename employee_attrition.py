# -*- coding: utf-8 -*-
"""Employee Attrition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WF2aQy3RnN6LRJrrlQSuGAZ1u_52csyw

# **EMPLOYEE ATTRITION**

**Employee attrition is the departure of employees from a company due to any reason including death and this is one of the most important fact to be considered in a company management. This ML based notebook is written building a better ML model to predict employee attrition**

The following are the columns in our dataset:

- employee id
- employee record date (year of data)
- birth date
- hire date
- termination date
- age
- length of service
- city
- department
- job title
- store number
- gender
- termination reason
- termination type
- status year
- status (LABEL)
- business unit

### Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""### Reading the Data"""

data = pd.read_csv("https://raw.githubusercontent.com/UmeshanUC/Employee-Attrition-Prediction/master/dataset.csv")
data[:][100:110].head()

data.shape

"""## **Data Analysis**

### Checking Null Values
"""

data.isnull().sum()

"""### Checking Data Info"""

data.info()

"""### Dropping Duplicates"""

data.drop_duplicates(inplace=True)
data.shape

"""### Dropping Un-important Columns

1.   'EmployeeID' and 'recorddate_key' does not affect the attrition behaviour
2.   'Age' affects. But 'birthdate_key' have no effect.
3.   There are 2 columns for Gender. Therefore 'gender_full' is removed.


"""

data.drop(['EmployeeID', 'birthdate_key', 'recorddate_key', 'gender_full'], axis=1, inplace=True)

"""### Get Unique Values per Column

#### City Names
"""

print(data.city_name.unique())

"""#### Department Names"""

print(data.department_name.unique())

"""#### Termination Reason Description"""

print(data.termreason_desc.unique())

"""#### Business Unit"""

print(data.BUSINESS_UNIT.unique())

"""#### Job Title"""

print(data.job_title.unique())

"""## **Visualization**

#### Termination Date Defaults
"""

data.terminationdate_key.value_counts()

"""'**1/1/1900**' shoud be replaced in data pre-processing

Distribution of Status
"""

data.STATUS.value_counts()

status_label = data.STATUS.value_counts()

plt.figure(figsize=(10, 5))

# Use plt.pie() to create a pie chart
plt.pie(status_label.values, labels=status_label.index, autopct='%1.1f%%')

plt.title('STATUS', fontsize=20)
plt.show()

"""We can observe that two classes are highly imbalanced

Distribution of Status based on City Name
"""

plt.figure(figsize=(10, 5))
sns.countplot(x="city_name", hue="STATUS", data=data)

"""-> Maximum number of employees are from the mega city in which the ratio of active to terminated status of the employee is higher (active > terminated)

-> Number of employees from the town are less compared to mega city but mugh higher compared to rural area and the ratio of active to terminated status of the employee is almost the same (active ~ terminated)

-> The least number of employees are from rural areas and the ratio of active to terminated status of the employee is lower (active < terminated)

Distribution of Status based on Business Unit
"""

plt.figure(figsize=(10, 5))
colors = ['purple', 'blue', 'green', 'orange']
sns.countplot(x="BUSINESS_UNIT", hue="STATUS", data=data, palette=colors)

"""-> More number of employees are from the stores and not from the head-office.

-> Cases of termination are higher in the head-office whereas cases of termination is lower in the stores.

Distribution of Status based on Job Title
"""

plt.figure(figsize=(10, 5))
colors = ['orange', 'green','purple', 'blue']
sns.countplot(x="job_title", hue="STATUS", data=data, palette=colors)

"""-> Maximum number of "Employees" are the ones with the employee job title and the number of ones with active status is more than the number of ones with terminated status.

Distribution of Status Year with respect to Status
"""

plt.figure(figsize=(10, 5))
plt.hist("STATUS_YEAR", data = data[data["STATUS"] == "ACTIVE"], alpha = 0.5, label = "ACTIVE",color='purple')
plt.hist("STATUS_YEAR", data = data[data["STATUS"] == "TERMINATED"], alpha = 0.5, label = "TERMINATED", color='green')
plt.title("Distribution of Status Year with respect to Status")
plt.xlabel("Status Year")
plt.legend(title = "STATUS")
plt.show()

"""Distribution of Length of Service with respect to Status"""

plt.figure(figsize=(10, 5))
plt.hist("length_of_service", data = data[data["STATUS"] == "ACTIVE"], alpha = 0.5, label = "ACTIVE", color='blue')
plt.hist("length_of_service", data = data[data["STATUS"] == "TERMINATED"], alpha = 0.5, label = "TERMINATED", color='red' )
plt.title("Distribution of Length of Service with respect to Status")
plt.xlabel("Length of Service")
plt.legend(title = "STATUS")
plt.show()

"""Distribution of Age with respect to Status"""

plt.figure(figsize=(10, 5))
plt.hist("age", data = data[data["STATUS"] == "ACTIVE"], alpha = 0.5, label = "ACTIVE", color='blue')
plt.hist("age", data = data[data["STATUS"] == "TERMINATED"], alpha = 0.5, label = "TERMINATED", color='red')
plt.title("Distribution of Age with respect to Status")
plt.xlabel("Age")
plt.legend(title = "STATUS")
plt.show()

"""Distribution Plot

## **Data Pre-Processing**

### Converting Job Titles into 5 Categories
"""

Executive_Leadership = ['CEO', 'VP Stores', 'Legal Counsel', 'VP Human Resources', 'VP Finance', 'CHief Information Officer']

Management = ['Store Manager', 'Meats Manager', 'Produce Manager', 'Bakery Manager', 'Customer Service Manager', 'Dairy Manager', 'Processed Foods Manager', 'Meat Cutter', 'Baker']


Human_Resources = ['Director, Recruitment', 'Director, Training', 'Director, Labor Relations', 'Director, HR Technology',
                   'Director, Employee Records', 'Director, Compensation', 'Recruiter', 'Labor Relations Analyst', 'HRIS Analyst',
                   'Benefits Admin', 'Compensation Analyst']

Accounting_and_Finance = ['Director, Accounts Receivable', 'Systems Analyst', 'Director, Accounts Payable', 'Director, Audit',
                          'Director, Accounting', 'Director, Investments', 'Corporate Lawyer',
                          'Accounts Receiveable Clerk', 'Accounts Payable Clerk', 'Auditor', 'Accounting Clerk', 'Investment Analyst']

Operations =['Exec Assistant, VP Stores', 'Exec Assistant, Legal Counsel', 'Exec Assistant, Human Resources',
             'Exec Assistant, Finance', 'Dairy Person', 'Produce Clerk', 'Shelf Stocker', 'Cashier', 'Trainer']

def job_title(job):
    if job in Executive_Leadership: return 'board'
    if job in Management: return 'manager'
    if job in Human_Resources: return 'HR'
    if job in Accounting_and_Finance: return 'financial accounting'
    if job in Operations: return 'operator'

data['job_title'] = data['job_title'].map(job_title)

"""#### Unique Values for Job Title after Reducing Data"""

print(data.job_title.unique())

"""### Changing the City Names

- Converting cities to either "Village" or "Town"

> Populations were obtained refering to  https://www.statcan.gc.ca/eng/start.


"""

city_population = {'Vancouver': 2400000,
                    'Victoria': 289625,
                    'Nanaimo': 84905,
                    'New Westminster': 58549,
                    'Kelowna': 125109,
                    'Burnaby': 202799,
                    'Kamloops': 68714,
                    'Prince George': 65558,
                    'Cranbrook': 18610,
                    'Surrey': 394976,
                    'Richmond': 182000,
                    'Terrace': 19443,
                    'Chilliwack': 77000,
                    'Trail': 9707,
                    'Langley': 23606,
                    'Vernon': 47274,
                    'Squamish': 19512,
                    'Quesnel': 13799,
                    'Abbotsford': 151683,
                    'North Vancouver': 48000,
                    'Fort St John': 17402,
                    'Williams Lake': 14168,
                    'West Vancouver': 42694,
                    'Port Coquitlam': 114565,
                    'Aldergrove': 12363,
                    'Fort Nelson': 3561,
                    'Nelson': 9813,
                    'New Westminister': 58549,
                    'Grand Forks': 4049,
                    'White Rock': 66450,
                    'Haney': 82256,
                    'Princeton': 2828,
                    'Dawson Creek': 10802,
                    'Bella Bella': 1019,
                    'Ocean Falls': 129,
                    'Pitt Meadows': 174410,
                    'Cortes Island': 1042,
                    'Valemount': 1021,
                    'Dease Lake': 335,
                    'Blue River': 157
                   }


def change_city_into_city_pop(city):
    return city_population(city)

data['city_name'] = data['city_name'].map(city_population)

def change_city(population):
    str = 'village'
    if (population >= 100000): str = 'town'
    return str

data['city_name'] = data.city_name.map(change_city)

"""#### Unique Values for Cities after Reducing Data"""

print(data['city_name'].unique())

"""### Upsampling"""

from sklearn.utils import resample

# Separate Target Classes
df_1 = data[data.STATUS=="ACTIVE"]
df_2 = data[data.STATUS=="TERMINATED"]

# Upsample minority class
df_2_upsampled = resample(df_2,
                                 replace=True,     # sample with replacement
                                 n_samples=48168,    # to match majority class
                                 random_state=123) # reproducible results

# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_1, df_2_upsampled])

# Display new class counts
df_upsampled.STATUS.value_counts()

"""#### Data after upsampling"""

status_label = df_upsampled.STATUS.value_counts()

plt.figure(figsize=(10, 5))

# Use plt.pie() to create a pie chart
plt.pie(status_label.values, labels=status_label.index, autopct='%1.1f%%')

plt.title('STATUS', fontsize=20)
plt.show()

"""Classes are balanced

### Mapping Numerical Values to Texts

#### City Name
> {'town': 0, 'village': 1}
"""

city_name_label = {value: key for key, value in enumerate(df_upsampled['city_name'].unique())}
df_upsampled['city_name'] = df_upsampled['city_name'].map(city_name_label)

"""#### Department Name
> {'Executive': 0, 'Store Management': 1, 'Meats': 2, 'Recruitment': 3, 'Training': 4, 'Labor Relations': 5, 'HR Technology': 6, 'Employee Records': 7, 'Compensation': 8, 'Legal': 9, 'Produce': 10, 'Accounts Receiveable': 11, 'Bakery': 12, 'Information Technology': 13, 'Accounts Payable': 14, 'Audit': 15, 'Accounting': 16, 'Investment': 17, 'Dairy': 18, 'Processed Foods': 19, 'Customer Service': 20}

"""

department_name_label = {value: key for key, value in enumerate(df_upsampled['department_name'].unique())}
df_upsampled['department_name'] = df_upsampled['department_name'].map(department_name_label)

"""#### Job Title
> {'board': 0, 'operator': 1, 'manager': 2, 'HR': 3, 'financial accounting': 4}
"""

job_title_label = {value: key for key, value in enumerate(df_upsampled['job_title'].unique())}
df_upsampled['job_title'] = df_upsampled['job_title'].map(job_title_label)

"""#### Gender
> {'M': 0, 'F': 1}
"""

gender_short_label = {value: key for key, value in enumerate(df_upsampled['gender_short'].unique())}
df_upsampled['gender_short'] = df_upsampled['gender_short'].map(gender_short_label)

"""#### Reason for Termination
> {'Not Applicable': 0, 'Resignaton': 1, 'Layoff': 2, 'Retirement': 3}
"""

term_desc_label = {value: key for key, value in enumerate(df_upsampled['termreason_desc'].unique())}
df_upsampled['termreason_desc'] = df_upsampled['termreason_desc'].map(term_desc_label)

"""#### Type of Termination
> {'Not Applicable': 0, 'Voluntary': 1, 'Involuntary': 2}
"""

termtype_desc_label = {value: key for key, value in enumerate(df_upsampled['termtype_desc'].unique())}
df_upsampled['termtype_desc'] = df_upsampled['termtype_desc'].map(termtype_desc_label)

"""#### Status
> {'ACTIVE': 0, 'TERMINATED': 1}
"""

status_label = {value: key for key, value in enumerate(df_upsampled['STATUS'].unique())}
df_upsampled['STATUS'] = df_upsampled['STATUS'].map(status_label)

"""#### Business Unit
> {'HEADOFFICE': 0, 'STORES': 1}
"""

business_label = {value: key for key, value in enumerate(df_upsampled['BUSINESS_UNIT'].unique())}
df_upsampled['BUSINESS_UNIT'] = df_upsampled['BUSINESS_UNIT'].map(business_label)

"""#### Hired Date and Termination Date
>  Number of days since January 1, 1 AD (as int)
"""

import datetime

def dateToi(dateStr):
  # Dates in string format ('2022/01/01')

  # Parse dates using datetime and convert to ordinal values
  date = datetime.datetime.strptime(dateStr, '%m/%d/%Y').toordinal()

  return date

df_upsampled['orighiredate_key']=df_upsampled['orighiredate_key'].map(dateToi)
df_upsampled['terminationdate_key']=df_upsampled['terminationdate_key'].map(dateToi)

df_upsampled.info()

"""### Replacing Nulls in Hired Date and Termination Date"""

hiredDateMean = int(df_upsampled['orighiredate_key'].mean())
teminationDateMean = int(df_upsampled['terminationdate_key'].mean())
def replaceNullDatesWithMean(dateInt, mean):
  if (dateInt == 693596): # 693596 represents 1/1/1900 in Number of days since January 1, 1 AD (as int)
    return mean
  else:
    return dateInt

df_upsampled['orighiredate_key'] = df_upsampled['orighiredate_key'].map(lambda dateInt:replaceNullDatesWithMean(dateInt, hiredDateMean))
df_upsampled['terminationdate_key'] = df_upsampled['terminationdate_key'].map(lambda dateInt:replaceNullDatesWithMean(dateInt, teminationDateMean))

df_upsampled.terminationdate_key.value_counts()

"""### Correlation Matrix"""

plt.figure(figsize=(15, 10))
sns.heatmap(df_upsampled.corr(), annot=False);
plt.title('Correlation Matrix', fontsize=24);

"""#### Checking the Correlation of Status with repect to other features"""

df_upsampled.corr()['STATUS'].sort_values(ascending=False)[1:]

"""- Matrix shows they are highly correlated each other. Need to remove one.
- 'termtype_desc' and 'termreason_desc' has a higher correlation to the STATUS which is the label. Higher the correlation to label is **better**.
- Therefore '**termreason_desc**' which less correlates to label is removed.
"""

df_upsampled[['termtype_desc', 'STATUS']]

"""#### Dropping highly correlated columns

- 'orighiredate_key' is removed as it highly correlated with 'length_of_service' while 'length_of_service' is highly correlated with label (desirable).
"""

df_upsampled.drop(['termreason_desc', 'orighiredate_key'], axis=1, inplace=True)

"""## **Binary Classification**

Binary classification was done using 3 models:
* Logistic Regression
* K-Nearest Neighbours Classifier
* Random Forest Classifier

The Classification Report, Confusion Matrix and Evaluation Metrics Accuracy, F1 Score and ROC-AUC Score are checked for each Classifier and are compared in order to decide which Classification Model is the best.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score,roc_curve,auc

accuracy_list = []
f1_list = []
roc_auc_list = []

"""Train Test Evaluation Function"""

def result(X, y, ts, rs, model):

    #train test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)

    #scaling
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)

    #fit on data
    model.fit(X_train, y_train)

    #prediction
    pred = model.predict(X_test)

    #performance of model
    print("Classification Report: \n", classification_report(y_test, pred))
    print("-" * 100)
    print()

    #accuracy of model
    acc = accuracy_score(y_test, pred)
    accuracy_list.append(acc)
    print("Accuracy Score: ", acc)
    print("-" * 100)
    print()

    #f1-score of model
    f1 = f1_score(y_test, pred)
    f1_list.append(f1)
    print("F1 Score: ", f1)
    print("-" * 100)
    print()

    #roc-auc curve of model
    fpr,tpr,threshold = roc_curve(y_test,pred)
    auc_value = auc(fpr,tpr)
    rocauc_score = roc_auc_score(y_test, pred)
    roc_auc_list.append(rocauc_score)
    plt.figure(figsize=(5,5),dpi=100)
    print("ROC-AUC Score: ", f1)
    print("-" * 100)
    print()
    plt.plot(fpr,tpr,linestyle='-',label = "(auc_value = %0.3f)" % auc_value)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()
    print()

    #confusion matrix for model
    print("Confusion Matrix: ")
    plt.figure(figsize=(10, 5))
    sns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='g', cmap='Blues');
    plt.title('Confusion Matrix', fontsize=20)

"""Independent and Dependent Features"""

df_upsampled.info()

# x = df_upsampled.drop(['STATUS'], axis=1)
# y = df_upsampled[['STATUS']]

x = df_upsampled[['terminationdate_key', 'age', 'length_of_service', 'city_name', 'department_name', 'job_title', 'store_name', 'gender_short',
        'STATUS_YEAR', 'BUSINESS_UNIT']]
y = df_upsampled[['STATUS']]

"""Logistic Regression"""

model = LogisticRegression()
result(x, y, 0.25, 42, model)

"""Random Forest Classifier"""

rf = RandomForestClassifier()
result(x, y, 0.25, 42, rf)

"""KNN Classifier"""

knn = KNeighborsClassifier()
result(x, y, 0.25, 42, knn)

dt = DecisionTreeClassifier()
result(x, y, 0.25, 42, dt)

"""Stochastic Gradient Descent Classifier

## **Classifier Comparison**
"""

classifier_list = ["Logistic Regression", "Random Forest", "KNN"]
list_class = []
for i in range(0, len(classifier_list)):
  listclass = [classifier_list[i], accuracy_list[i], f1_list[i], roc_auc_list[i]]
  list_class.append(listclass)

# list_class

cc_table = pd.DataFrame(list_class, columns = ["Classifier", "Accuracy", "F1 Score", "ROC-AUC Score"])
cc_table.sort_values(ascending = False, by = "Accuracy")

"""Plotting the Comparison Table as a Bar Plot

For Accuracy
"""

plt.figure(figsize = (8,6))
sns.barplot(x = cc_table["Accuracy"]*100,
            y = cc_table["Classifier"],
            data = cc_table,
            order = cc_table.sort_values("Accuracy", ascending = False).Classifier)

"""For F1 Score

For ROC-AUC Score

**Conclusion: After comparing the evaluation metrics of each model, we can conclude that Random Forest Classifier seems to be the best classifier that can be used and Decision Tree would be the next best option.**
"""